<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Advanced Usage Guide · ForwardDiff</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>ForwardDiff</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../index.html">Introduction</a></li><li><span class="toctext">User Documentation</span><ul><li><a class="toctext" href="limitations.html">Limitations of ForwardDiff</a></li><li><a class="toctext" href="api.html">Differentiation API</a></li><li class="current"><a class="toctext" href="advanced.html">Advanced Usage Guide</a><ul class="internal"><li><a class="toctext" href="#Retrieving-Lower-Order-Results-1">Retrieving Lower-Order Results</a></li><li><a class="toctext" href="#Configuring-Chunk-Size-1">Configuring Chunk Size</a></li><li><a class="toctext" href="#Fixing-NaN/Inf-Issues-1">Fixing NaN/Inf Issues</a></li><li><a class="toctext" href="#Hessian-of-a-vector-valued-function-1">Hessian of a vector-valued function</a></li><li><a class="toctext" href="#SIMD-Vectorization-1">SIMD Vectorization</a></li><li><a class="toctext" href="#Custom-tags-and-tag-checking-1">Custom tags and tag checking</a></li></ul></li><li><a class="toctext" href="upgrade.html">Upgrading from Older Versions</a></li></ul></li><li><span class="toctext">Developer Documentation</span><ul><li><a class="toctext" href="../dev/how_it_works.html">How ForwardDiff Works</a></li><li><a class="toctext" href="../dev/contributing.html">How to Contribute</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>User Documentation</li><li><a href="advanced.html">Advanced Usage Guide</a></li></ul><a class="edit-page" href="https://github.com/JuliaDiff/ForwardDiff.jl/blob/master/docs/src/user/advanced.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Advanced Usage Guide</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Advanced-Usage-Guide-1" href="#Advanced-Usage-Guide-1">Advanced Usage Guide</a></h1><p>This document describes several techniques and features that can be used in conjunction ForwardDiff&#39;s basic API in order to fine-tune calculations and increase performance.</p><h2><a class="nav-anchor" id="Retrieving-Lower-Order-Results-1" href="#Retrieving-Lower-Order-Results-1">Retrieving Lower-Order Results</a></h2><p>Let&#39;s say you want to calculate the value, gradient, and Hessian of some function <code>f</code> at an input <code>x</code>. You could execute <code>f(x)</code>, <code>ForwardDiff.gradient(f, x)</code> and <code>ForwardDiff.hessian(f, x)</code>, but that would be a <strong>horribly redundant way to accomplish this task!</strong></p><p>In the course of calculating higher-order derivatives, ForwardDiff ends up calculating all the lower-order derivatives and primal value <code>f(x)</code>. To retrieve these results in one fell swoop, you can utilize the <a href="https://github.com/JuliaDiff/DiffResults.jl">DiffResults</a> API.</p><p>All mutating ForwardDiff API methods support the DiffResults API. In other words, API methods of the form <code>ForwardDiff.method!(out, args...)</code> will work appropriately if <code>isa(out, DiffResults.DiffResult)</code>.</p><h2><a class="nav-anchor" id="Configuring-Chunk-Size-1" href="#Configuring-Chunk-Size-1">Configuring Chunk Size</a></h2><p>ForwardDiff performs partial derivative evaluation on one &quot;chunk&quot; of the input vector at a time. Each differentiation of a chunk requires a call to the target function as well as additional memory proportional to the square of the chunk&#39;s size. Thus, a smaller chunk size makes better use of memory bandwidth at the cost of more calls to the target function, while a larger chunk size reduces calls to the target function at the cost of more memory bandwidth.</p><p>For example:</p><pre><code class="language-julia">julia&gt; using ForwardDiff: GradientConfig, Chunk, gradient!

# let&#39;s use a Rosenbrock function as our target function
julia&gt; function rosenbrock(x)
           a = one(eltype(x))
           b = 100 * a
           result = zero(eltype(x))
           for i in 1:length(x)-1
               result += (a - x[i])^2 + b*(x[i+1] - x[i]^2)^2
           end
           return result
       end
rosenbrock (generic function with 1 method)

# input vector
julia&gt; x = rand(10000);

# output buffer
julia&gt; out = similar(x);

# construct GradientConfig with chunk size of 1
julia&gt; cfg1 = GradientConfig(rosenbrock, x, Chunk{1}());

# construct GradientConfig with chunk size of 4
julia&gt; cfg4 = GradientConfig(rosenbrock, x, Chunk{4}());

# construct GradientConfig with chunk size of 10
julia&gt; cfg10 = GradientConfig(rosenbrock, x, Chunk{10}());

# (input length of 10000) / (chunk size of 1) = (10000 1-element chunks)
julia&gt; @time gradient!(out, rosenbrock, x, cfg1);
  0.775139 seconds (4 allocations: 160 bytes)

# (input length of 10000) / (chunk size of 4) = (2500 4-element chunks)
julia&gt; @time gradient!(out, rosenbrock, x, cfg4);
  0.386459 seconds (4 allocations: 160 bytes)

# (input length of 10000) / (chunk size of 10) = (1000 10-element chunks)
julia&gt; @time gradient!(out, rosenbrock, x, cfg10);
  0.282529 seconds (4 allocations: 160 bytes)</code></pre><p>If you do not explicity provide a chunk size, ForwardDiff will try to guess one for you based on your input vector:</p><pre><code class="language-julia"># The GradientConfig constructor will automatically select a
# chunk size in one is not explicitly provided
julia&gt; cfg = ForwardDiff.GradientConfig(rosenbrock, x);

julia&gt; @time ForwardDiff.gradient!(out, rosenbrock, x, cfg);
  0.281853 seconds (4 allocations: 160 bytes)</code></pre><p>If your input dimension is constant across calls, you should explicitly select a chunk size rather than relying on ForwardDiff&#39;s heuristic. There are two reasons for this. The first is that ForwardDiff&#39;s heuristic depends only on the input dimension, whereas in reality the optimal chunk size will also depend on the target function. The second is that ForwardDiff&#39;s heuristic is inherently type-unstable, which can cause the entire call to be type-unstable.</p><p>If your input dimension is a runtime variable, you can rely on ForwardDiff&#39;s heuristic without sacrificing type stability by manually asserting the output type, or - even better - by using the in-place API functions:</p><pre><code class="language-julia"># will be type-stable since you&#39;re asserting the output type
ForwardDiff.gradient(rosenbrock, x)::Vector{Float64}

# will be type-stable since `out` is returned, and Julia knows the type of `out`
ForwardDiff.gradient!(out, rosenbrock, x)</code></pre><p>One final question remains: How should you select a chunk size? The answer is essentially &quot;perform your own benchmarks and see what works best for your use case.&quot; As stated before, the optimal chunk size is heavily dependent on the target function and length of the input vector.</p><p>Note that it is usually best to pick a chunk size which divides evenly into the input dimension. Otherwise, ForwardDiff has to construct and utilize an extra &quot;remainder&quot; chunk to complete the calculation.</p><h2><a class="nav-anchor" id="Fixing-NaN/Inf-Issues-1" href="#Fixing-NaN/Inf-Issues-1">Fixing NaN/Inf Issues</a></h2><p>ForwardDiff&#39;s default behavior is to return <code>NaN</code> for undefined derivatives (or otherwise mirror the behavior of the function in <code>Base</code>, if it would return an error). This is usually the correct thing to do, but in some cases can erroneously &quot;poison&quot; values which aren&#39;t sensitive to the input and thus cause ForwardDiff to incorrectly return <code>NaN</code> or <code>Inf</code> derivatives. For example:</p><pre><code class="language-julia"># the dual number&#39;s perturbation component is zero, so this
# variable should not propagate derivative information
julia&gt; log(ForwardDiff.Dual{:tag}(0.0, 0.0))
Dual{:tag}(-Inf,NaN) # oops, this NaN should be 0.0</code></pre><p>Here, ForwardDiff computes the derivative of <code>log(0.0)</code> as <code>NaN</code> and then propagates this derivative by multiplying it by the perturbation component. Usually, ForwardDiff can rely on the identity <code>x * 0.0 == 0.0</code> to prevent the derivatives from propagating when the perturbation component is <code>0.0</code>. However, this identity doesn&#39;t hold if <code>isnan(y)</code> or <code>isinf(y)</code>, in which case a <code>NaN</code> derivative will be propagated instead.</p><p>It is possible to fix this behavior by checking that the perturbation component is zero before attempting to propagate derivative information, but this check can noticeably decrease performance (~5%-10% on our benchmarks).</p><p>In order to preserve performance in the majority of use cases, ForwardDiff disables this check by default. If your code is affected by this <code>NaN</code> behvaior, you can enable ForwardDiff&#39;s <code>NaN</code>-safe mode by setting the <code>NANSAFE_MODE_ENABLED</code> constant to <code>true</code> in ForwardDiff&#39;s source.</p><p>In the future, we plan on allowing users and downstream library authors to dynamically enable <a href="https://github.com/JuliaDiff/ForwardDiff.jl/issues/181">NaN<code>-safe mode via the</code>AbstractConfig` API</a>.</p><h2><a class="nav-anchor" id="Hessian-of-a-vector-valued-function-1" href="#Hessian-of-a-vector-valued-function-1">Hessian of a vector-valued function</a></h2><p>While ForwardDiff does not have a built-in function for taking Hessians of vector-valued functions, you can easily compose calls to <code>ForwardDiff.jacobian</code> to accomplish this. For example:</p><pre><code class="language-julia">julia&gt; ForwardDiff.jacobian(x -&gt; ForwardDiff.jacobian(cumprod, x), [1,2,3])
9×3 Array{Int64,2}:
 0  0  0
 0  1  0
 0  3  2
 0  0  0
 1  0  0
 3  0  1
 0  0  0
 0  0  0
 2  1  0</code></pre><p>Since this functionality is composed from ForwardDiff&#39;s existing API rather than built into it, you&#39;re free to construct a <code>vector_hessian</code> function which suits your needs. For example, if you require the shape of the output to be a tensor rather than a block matrix, you can do so with a <code>reshape</code> (note that <code>reshape</code> does not copy data, so it&#39;s not an expensive operation):</p><pre><code class="language-julia">julia&gt; function vector_hessian(f, x)
       n = length(x)
       out = ForwardDiff.jacobian(x -&gt; ForwardDiff.jacobian(f, x), x)
       return reshape(out, n, n, n)
   end
vector_hessian (generic function with 1 method)

julia&gt; vector_hessian(cumprod, [1, 2, 3])
3×3×3 Array{Int64,3}:
[:, :, 1] =
 0  0  0
 0  1  0
 0  3  2

[:, :, 2] =
 0  0  0
 1  0  0
 3  0  1

[:, :, 3] =
 0  0  0
 0  0  0
 2  1  0</code></pre><p>Likewise, you could write a version of <code>vector_hessian</code> which supports functions of the form <code>f!(y, x)</code>, or perhaps an in-place Jacobian with <code>ForwardDiff.jacobian!</code>.</p><h2><a class="nav-anchor" id="SIMD-Vectorization-1" href="#SIMD-Vectorization-1">SIMD Vectorization</a></h2><p>Many operations on ForwardDiff&#39;s dual numbers are amenable to <a href="https://en.wikipedia.org/wiki/SIMD#Hardware">SIMD vectorization</a>. For some ForwardDiff benchmarks, we&#39;ve seen SIMD vectorization yield <a href="https://github.com/JuliaDiff/ForwardDiff.jl/issues/98#issuecomment-253149761">speedups of almost 3x</a>.</p><p>To enable SIMD optimizations, start your Julia process with the <code>-O3</code> flag. This flag enables <a href="http://llvm.org/docs/Vectorizers.html#the-slp-vectorizer">LLVM&#39;s SLPVectorizerPass</a> during compilation, which attempts to automatically insert SIMD instructions where possible for certain arithmetic operations.</p><p>Here&#39;s an example of LLVM bitcode generated for an addition of two <code>Dual</code> numbers without SIMD instructions (i.e. not starting Julia with <code>-O3</code>):</p><pre><code class="language-julia">julia&gt; using ForwardDiff: Dual

julia&gt; a = Dual(1., 2., 3., 4.)
Dual{Nothing}(1.0,2.0,3.0,4.0)

julia&gt; b = Dual(5., 6., 7., 8.)
Dual{Nothing}(5.0,6.0,7.0,8.0)

julia&gt; @code_llvm a + b

define void @&quot;julia_+_70852&quot;(%Dual* noalias sret, %Dual*, %Dual*) #0 {
top:
  %3 = getelementptr inbounds %Dual, %Dual* %1, i64 0, i32 1, i32 0, i64 0
  %4 = load double, double* %3, align 8
  %5 = getelementptr inbounds %Dual, %Dual* %2, i64 0, i32 1, i32 0, i64 0
  %6 = load double, double* %5, align 8
  %7 = fadd double %4, %6
  %8 = getelementptr inbounds %Dual, %Dual* %1, i64 0, i32 1, i32 0, i64 1
  %9 = load double, double* %8, align 8
  %10 = getelementptr inbounds %Dual, %Dual* %2, i64 0, i32 1, i32 0, i64 1
  %11 = load double, double* %10, align 8
  %12 = fadd double %9, %11
  %13 = getelementptr inbounds %Dual, %Dual* %1, i64 0, i32 1, i32 0, i64 2
  %14 = load double, double* %13, align 8
  %15 = getelementptr inbounds %Dual, %Dual* %2, i64 0, i32 1, i32 0, i64 2
  %16 = load double, double* %15, align 8
  %17 = fadd double %14, %16
  %18 = getelementptr inbounds %Dual, %Dual* %1, i64 0, i32 0
  %19 = load double, double* %18, align 8
  %20 = getelementptr inbounds %Dual, %Dual* %2, i64 0, i32 0
  %21 = load double, double* %20, align 8
  %22 = fadd double %19, %21
  %23 = getelementptr inbounds %Dual, %Dual* %0, i64 0, i32 0
  store double %22, double* %23, align 8
  %24 = getelementptr inbounds %Dual, %Dual* %0, i64 0, i32 1, i32 0, i64 0
  store double %7, double* %24, align 8
  %25 = getelementptr inbounds %Dual, %Dual* %0, i64 0, i32 1, i32 0, i64 1
  store double %12, double* %25, align 8
  %26 = getelementptr inbounds %Dual, %Dual* %0, i64 0, i32 1, i32 0, i64 2
  store double %17, double* %26, align 8
  ret void
}</code></pre><p>If we start up Julia with <code>-O3</code> instead, the call to <code>@code_llvm</code> will show that LLVM can SIMD-vectorize the addition:</p><pre><code class="language-julia">julia&gt; @code_llvm a + b

define void @&quot;julia_+_70842&quot;(%Dual* noalias sret, %Dual*, %Dual*) #0 {
top:
  %3 = bitcast %Dual* %1 to &lt;4 x double&gt;*            # cast the Dual to a SIMD-able LLVM vector
  %4 = load &lt;4 x double&gt;, &lt;4 x double&gt;* %3, align 8
  %5 = bitcast %Dual* %2 to &lt;4 x double&gt;*
  %6 = load &lt;4 x double&gt;, &lt;4 x double&gt;* %5, align 8
  %7 = fadd &lt;4 x double&gt; %4, %6                      # SIMD add
  %8 = bitcast %Dual* %0 to &lt;4 x double&gt;*
  store &lt;4 x double&gt; %7, &lt;4 x double&gt;* %8, align 8
  ret void
}</code></pre><p>Note that whether or not SIMD instructions can actually be used will depend on your machine and Julia build. For example, pre-built Julia binaries might not emit vectorized LLVM bitcode. To overcome this specific issue, you can <a href="http://docs.julialang.org/en/latest/devdocs/sysimg">locally rebuild Julia&#39;s system image</a>.</p><h2><a class="nav-anchor" id="Custom-tags-and-tag-checking-1" href="#Custom-tags-and-tag-checking-1">Custom tags and tag checking</a></h2><p>The <code>Dual</code> type includes a &quot;tag&quot; parameter indicating the particular function call to which it belongs. This is to avoid a problem known as <a href="https://github.com/JuliaDiff/ForwardDiff.jl/issues/83">_perturbation confusion_</a> which can arise when there are nested differentiation calls. Tags are automatically generated as part of the appropriate config object, and the tag is checked when the config is used as part of a differentiation call (<code>derivative</code>, <code>gradient</code>, etc.): an <code>InvalidTagException</code> will be thrown if the incorrect config object is used.</p><p>This checking can sometimes be inconvenient, and there are certain cases where you may want to disable this checking.</p><div class="admonition warning"><div class="admonition-title">Warning</div><div class="admonition-text"><p>Disabling tag checking should only be done with caution, especially if the code itself could be used inside another differentiation call.</p></div></div><ol><li><p>(preferred) Provide an extra <code>Val{false}()</code> argument to the differentiation function, e.g.</p><pre><code class="language-julia">cfg = ForwarDiff.GradientConfig(g, x)
ForwarDiff.gradient(f, x, cfg, Val{false}())</code></pre><p>If using as part of a library, the tag can be checked manually via</p><pre><code class="language-julia">ForwardDiff.checktag(cfg, g, x)</code></pre></li><li><p>(discouraged) Construct the config object with <code>nothing</code> instead of a function argument, e.g.</p><pre><code class="language-julia">cfg = GradientConfig(nothing, x)
gradient(f, x, cfg)</code></pre></li></ol><footer><hr/><a class="previous" href="api.html"><span class="direction">Previous</span><span class="title">Differentiation API</span></a><a class="next" href="upgrade.html"><span class="direction">Next</span><span class="title">Upgrading from Older Versions</span></a></footer></article></body></html>
